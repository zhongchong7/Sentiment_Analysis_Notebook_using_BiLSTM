{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1799ef0",
   "metadata": {},
   "source": [
    "Environment Setup and Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df820bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(42)\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274a07cd",
   "metadata": {},
   "source": [
    "Text preprocessing and word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17797ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenize function\n",
    "def tokenize(text):\n",
    "    # convert text to lower case\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    return text.split()\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data_path, vocab=None, max_len=500):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # read data from a local file\n",
    "        for label, sentiment in enumerate(['neg', 'pos']):\n",
    "            path = os.path.join(data_path, sentiment)\n",
    "            if not os.path.exists(path): continue\n",
    "            for file in os.listdir(path):\n",
    "                with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "                    self.data.append(tokenize(f.read()))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "        if vocab is None:\n",
    "            all_tokens = [token for text in self.data for token in text]\n",
    "            counts = Counter(all_tokens)\n",
    "            # filter out rare words and add pad and unk token\n",
    "            self.vocab = {token: i for i, token in enumerate(['<pad>', '<unk>'] + [token for token, count in counts.items() if count >= 5])}\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "        # convert tokens to indices\n",
    "        self.data = [[self.vocab.get(token, self.vocab['<unk>']) for token in text] for text in self.data]\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.data[idx]\n",
    "        # Truncate and pad sequences\n",
    "        indices = indices[:self.max_len]\n",
    "        indices += [0] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Instantiate dataset (replace 'aclImdb/train' with your local path)\n",
    "full_train_ds = IMDBDataset('aclImdb/train')\n",
    "\n",
    "print('train size:', len(full_train_ds))\n",
    "print('pos labels:', sum(full_train_ds.labels), 'total:', len(full_train_ds.labels))\n",
    "\n",
    "# Train/validation split\n",
    "train_size = int(0.9 * len(full_train_ds))\n",
    "val_size = len(full_train_ds) - train_size\n",
    "train_ds, val_ds = random_split(\n",
    "    full_train_ds,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "# Test set (reuse training vocabulary)\n",
    "test_ds = IMDBDataset('aclImdb/test', vocab=full_train_ds.vocab)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82850589",
   "metadata": {},
   "source": [
    "Load GloVe Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe9c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_matrix(glove_path, vocab, emb_dim=100):\n",
    "    \"\"\"\n",
    "    glove_path: path to 'glove.6B.100d.txt'\n",
    "    \"\"\"\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    \n",
    "    for word, i in vocab.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            # Initialize missing words randomly\n",
    "            embedding_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim,))\n",
    "            \n",
    "    return torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "# Prepare GloVe weights\n",
    "EMB_DIM = 100  # Must match the GloVe dimension\n",
    "glove_path = 'glove.6B.100d.txt'\n",
    "glove_weights = get_glove_matrix(glove_path, full_train_ds.vocab, emb_dim=EMB_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9a618a",
   "metadata": {},
   "source": [
    "Define RNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d9e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, output_dim, glove_weights):\n",
    "        super().__init__()\n",
    "        # 1. Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.embedding.weight.data.copy_(glove_weights)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # 2. BiLSTM layer\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # 3. Classification head\n",
    "        self.fc = nn.Linear(hid_dim * 4, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text: [batch, seq_len]\n",
    "        embedded = self.dropout(self.embedding(text)) # [batch, seq_len, emb_dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        # mean + max pooling\n",
    "        avg_pool = output.mean(dim=1)\n",
    "        max_pool, _ = output.max(dim=1)\n",
    "        feat = torch.cat((avg_pool, max_pool), dim=1)  # [batch, hid_dim*4]\n",
    "        return self.fc(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deaf06b",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a68f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "HIDDEN_DIM = 256\n",
    "# Assumes train_loader and glove_weights are ready\n",
    "model = SentimentRNN(len(full_train_ds.vocab), EMB_DIM, HIDDEN_DIM, 2, glove_weights).to(device)\n",
    "\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.train()\n",
    "    \n",
    "    for texts, labels in loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(texts)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        preds = predictions.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    avg_loss = epoch_loss / len(loader)\n",
    "    acc = correct / total if total > 0 else 0\n",
    "    return avg_loss, acc\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            predictions = model(texts)\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            preds = predictions.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    avg_loss = epoch_loss / len(loader)\n",
    "    acc = correct / total if total > 0 else 0\n",
    "    return avg_loss, acc\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    print(\n",
    "        f'Epoch: {epoch+1:02}, '\n",
    "        f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "        f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6660ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the performance\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(epochs, train_losses, label='Train Loss')\n",
    "plt.plot(epochs, val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(epochs, train_accs, label='Train Acc')\n",
    "plt.plot(epochs, val_accs, label='Val Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNNCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
